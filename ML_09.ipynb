{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Multi-Layer Perceptron (MLP) from scratch\n",
        "# Demonstration of Backpropagation and Weight Updates\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Activation function: Sigmoid and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Sample XOR dataset\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Initialize weights and biases\n",
        "input_neurons = 2\n",
        "hidden_neurons = 2\n",
        "output_neurons = 1\n",
        "\n",
        "# Random weight initialization\n",
        "W1 = np.random.uniform(size=(input_neurons, hidden_neurons))\n",
        "b1 = np.random.uniform(size=(1, hidden_neurons))\n",
        "W2 = np.random.uniform(size=(hidden_neurons, output_neurons))\n",
        "b2 = np.random.uniform(size=(1, output_neurons))\n",
        "\n",
        "# Learning rate\n",
        "lr = 0.5\n",
        "epochs = 10000\n",
        "\n",
        "print(\"Initial Weights and Biases:\")\n",
        "print(\"W1:\", W1)\n",
        "print(\"b1:\", b1)\n",
        "print(\"W2:\", W2)\n",
        "print(\"b2:\", b2)\n",
        "print(\"--------------------------------------------------\")\n",
        "\n",
        "# Training Process\n",
        "for epoch in range(epochs):\n",
        "    # Forward Pass\n",
        "    hidden_input = np.dot(X, W1) + b1\n",
        "    hidden_output = sigmoid(hidden_input)\n",
        "\n",
        "    final_input = np.dot(hidden_output, W2) + b2\n",
        "    final_output = sigmoid(final_input)\n",
        "\n",
        "    # Calculate Error\n",
        "    error = y - final_output\n",
        "\n",
        "    # Backpropagation\n",
        "    d_output = error * sigmoid_derivative(final_output)\n",
        "    error_hidden = d_output.dot(W2.T)\n",
        "    d_hidden = error_hidden * sigmoid_derivative(hidden_output)\n",
        "\n",
        "    # Weight updates\n",
        "    W2_update = hidden_output.T.dot(d_output) * lr\n",
        "    W1_update = X.T.dot(d_hidden) * lr\n",
        "\n",
        "    W2 += W2_update\n",
        "    b2 += np.sum(d_output, axis=0, keepdims=True) * lr\n",
        "    W1 += W1_update\n",
        "    b1 += np.sum(d_hidden, axis=0, keepdims=True) * lr\n",
        "\n",
        "    # Print weight changes occasionally\n",
        "    if epoch % 2000 == 0:\n",
        "        print(f\"Epoch {epoch}:\")\n",
        "        print(\"Error:\", np.mean(np.abs(error)))\n",
        "        print(\"ΔW1:\", W1_update)\n",
        "        print(\"ΔW2:\", W2_update)\n",
        "        print(\"--------------------------------------------------\")\n",
        "\n",
        "print(\"\\nFinal Outputs after training:\")\n",
        "print(final_output)\n",
        "\n",
        "print(\"\\nFinal Weights and Biases:\")\n",
        "print(\"W1:\", W1)\n",
        "print(\"b1:\", b1)\n",
        "print(\"W2:\", W2)\n",
        "print(\"b2:\", b2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r--jVIMs_cXK",
        "outputId": "2dcf262b-9770-4cb0-ba93-f2b34bfa0d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Weights and Biases:\n",
            "W1: [[0.37454012 0.95071431]\n",
            " [0.73199394 0.59865848]]\n",
            "b1: [[0.15601864 0.15599452]]\n",
            "W2: [[0.05808361]\n",
            " [0.86617615]]\n",
            "b2: [[0.60111501]]\n",
            "--------------------------------------------------\n",
            "Epoch 0:\n",
            "Error: 0.4977550305860017\n",
            "ΔW1: [[-0.00039288 -0.00427296]\n",
            " [-0.00040377 -0.00351079]]\n",
            "ΔW2: [[-0.06239121]\n",
            " [-0.06500368]]\n",
            "--------------------------------------------------\n",
            "Epoch 2000:\n",
            "Error: 0.05041354665388302\n",
            "ΔW1: [[0.00035961 0.00027747]\n",
            " [0.00035798 0.00026657]]\n",
            "ΔW2: [[-0.00082984]\n",
            " [ 0.00077502]]\n",
            "--------------------------------------------------\n",
            "Epoch 4000:\n",
            "Error: 0.029990120209400284\n",
            "ΔW1: [[1.23363912e-04 1.01259971e-04]\n",
            " [1.22832317e-04 9.83729001e-05]]\n",
            "ΔW2: [[-0.00030856]\n",
            " [ 0.00030267]]\n",
            "--------------------------------------------------\n",
            "Epoch 6000:\n",
            "Error: 0.023140942472583994\n",
            "ΔW1: [[7.20145776e-05 6.04914611e-05]\n",
            " [7.17216340e-05 5.90173962e-05]]\n",
            "ΔW2: [[-0.00018757]\n",
            " [ 0.00018669]]\n",
            "--------------------------------------------------\n",
            "Epoch 8000:\n",
            "Error: 0.019460852596286334\n",
            "ΔW1: [[5.02120591e-05 4.27385077e-05]\n",
            " [5.00173319e-05 4.17994996e-05]]\n",
            "ΔW2: [[-0.00013436]\n",
            " [ 0.00013468]]\n",
            "--------------------------------------------------\n",
            "\n",
            "Final Outputs after training:\n",
            "[[0.01890475]\n",
            " [0.98371361]\n",
            " [0.98369334]\n",
            " [0.01686123]]\n",
            "\n",
            "Final Weights and Biases:\n",
            "W1: [[4.59244504 6.47246975]\n",
            " [4.5971031  6.49153682]]\n",
            "b1: [[-7.05239171 -2.8842842 ]]\n",
            "W2: [[-10.32676834]\n",
            " [  9.62121009]]\n",
            "b2: [[-4.44969307]]\n"
          ]
        }
      ]
    }
  ]
}